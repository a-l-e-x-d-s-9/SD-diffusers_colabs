{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rQe0y8Hj8geV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title SETUP\n",
        "!pip install git+https://github.com/huggingface/diffusers.git transformers xformers accelerate omegaconf torchsde\n",
        "!apt -y install -qq aria2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1c36S2I78rlP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(f'models',exist_ok=True)\n",
        "\n",
        "# @title DOWNLOAD MODEL\n",
        "\n",
        "Model_url = 'https://civitai.com/api/download/models/155282?type=Model&format=SafeTensor&size=pruned&fp=fp16'# @param {type:\"string\"}\n",
        "Model_name = 'Sassy_Girls'# @param {type:\"string\"}\n",
        "if not Model_name.endswith('.safetensors'):\n",
        "    Model_name=Model_name+'.safetensors'\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"{Model_url}\" -d models -o {Model_name}\n",
        "print('\\nAvailable models :')\n",
        "for MODEL in os.listdir('models'):\n",
        "    print(MODEL.replace('.safetensors',''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EMBEDDING_URL SHOULD BE A URL TO A '.safetensors' FILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title LOAD EMBEDDING\n",
        "\n",
        "EMBEDDING_URL= ''# @param {type:\"string\"}\n",
        "embed = req.get(EMBEDDING_URL)\n",
        "filename = embed.headers['Content-Disposition'].split('filename=')[1].strip('\"')\n",
        "tk = filename.strip('.safetensors')\n",
        "with open(filename,\"wb\") as r:\n",
        "    r.write(embed.content)\n",
        "state_dict = load_file(filename)\n",
        "try:\n",
        "    pipe.load_textual_inversion(state_dict[\"clip_l\"], token=tk, text_encoder=pipe.text_encoder, tokenizer=pipe.tokenizer)\n",
        "    print(f\"Successfuly Loaded\\n\\nTrigger Keyword :\\n{tk}\")\n",
        "except Exception as e:\n",
        "    em = str(e)\n",
        "    if f\"Token {tk} already\" in em:\n",
        "        print(f\"Successfuly Loaded\\n\\nTrigger Keyword :\\n{tk}\")\n",
        "    else:\n",
        "        print(em)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LOADING MANY LORAs IS NOT SUGGESTED AS COLAB HAS LIMITED RESOURCES AND IT MIGHT CRASH , 1-2 LORAs WORK FINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title LOAD LORA\n",
        "LORA_NAME= ''# @param {type:\"string\"}\n",
        "LORA_SCALE  = 0.5# @param {type:\"number\"}\n",
        "if not LORA_NAME.endswith('.safetensors'):\n",
        "    LORA_NAME=LORA_NAME+'.safetensors'\n",
        "print(f'\\n\\nCurrently selected : {LORA_NAME}')\n",
        "lora_path = f'models/{LORA_NAME}'\n",
        "pipe.load_lora_weights(lora_path)\n",
        "pipe.fuse_lora(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ViqGxOCo9UeC"
      },
      "outputs": [],
      "source": [
        "# @title LOAD MODEL\n",
        "\n",
        "from diffusers import StableDiffusionPipeline, DDIMParallelScheduler, DDIMScheduler, DDPMParallelScheduler, DDPMScheduler, DEISMultistepScheduler, DPMSolverMultistepScheduler, DPMSolverSDEScheduler, DPMSolverSinglestepScheduler, EulerAncestralDiscreteScheduler, EulerDiscreteScheduler, HeunDiscreteScheduler, KDPM2AncestralDiscreteScheduler, KDPM2DiscreteScheduler, LMSDiscreteScheduler, PNDMScheduler, UniPCMultistepScheduler\n",
        "from diffusers.utils import make_image_grid\n",
        "import torch,random,os\n",
        "from IPython.display import display\n",
        "\n",
        "os.makedirs(f'outputs',exist_ok=True)\n",
        "MODEL_NAME  = \"Sassy_Girls\"# @param {type:\"string\"}\n",
        "LOAD_TYPE = \"FP16 (FAST BUT AVERAGE QUALITY)\"# @param [\"FP32 (SLOW BUT GOOD QUALITY)\",\"FP16 (FAST BUT AVERAGE QUALITY)\"]\n",
        "if not MODEL_NAME.endswith('.safetensors'):\n",
        "    MODEL_NAME=MODEL_NAME+'.safetensors'\n",
        "print(f'\\n\\nCurrently selected : {MODEL_NAME}')\n",
        "model_path = f'models/{MODEL_NAME}'\n",
        "if LOAD_TYPE == \"FP32 (SLOW BUT GOOD QUALITY)\":\n",
        "    pipe = StableDiffusionPipeline.from_single_file(model_path,load_safety_checker = False,scheduler_type='dpm')\n",
        "else:\n",
        "    pipe = StableDiffusionPipeline.from_single_file(model_path, revision=\"fp16\",load_safety_checker = False, torch_dtype=torch.float16,variant=\"fp16\",scheduler_type='dpm')\n",
        "pipe.to(\"cuda\")\n",
        "pipe.enable_xformers_memory_efficient_attention()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qAetP2pwAOJr"
      },
      "outputs": [],
      "source": [
        "# @title GENERATE IMAGE\n",
        "from PIL import PngImagePlugin\n",
        "\n",
        "PROMPT= ''# @param {type:\"string\"}\n",
        "NEGATIVE_PROMPT = ''# @param {type:\"string\"}\n",
        "WIDTH  = 512# @param {type:\"integer\"}\n",
        "HEIGHT = 512 # @param {type:\"integer\"}\n",
        "SAMPLING_STEPS  = 20# @param {type:\"integer\"}\n",
        "CFG_scale  = 7# @param {type:\"number\"}\n",
        "SEED  = -1# @param {type:\"number\"}\n",
        "CLIP_SKIP  = 1# @param {type:\"number\"}\n",
        "SCHEDULER  = \"DPMSolverMultistepScheduler\"# @param [\"DDIMParallelScheduler\",\"DDIMScheduler\",\"DDPMParallelScheduler\",\"DDPMScheduler\",\"DEISMultistepScheduler\",\"DPMSolverMultistepScheduler\",\"DPMSolverSDEScheduler\",\"DPMSolverSinglestepScheduler\",\"EulerAncestralDiscreteScheduler\",\"EulerDiscreteScheduler\",\"HeunDiscreteScheduler\",\"KDPM2AncestralDiscreteScheduler\",\"KDPM2DiscreteScheduler\",\"LMSDiscreteScheduler\",\"PNDMScheduler\",\"UniPCMultistepScheduler\"]\n",
        "USE_KARRAS = True # @param {type:\"boolean\"}\n",
        "NUMBER_OF_IMAGES  = 1# @param {type:\"number\"}\n",
        "CLIP_SKIP = None if CLIP_SKIP == 1 else CLIP_SKIP - 1\n",
        "\n",
        "# Schedulers mapping: https://huggingface.co/docs/diffusers/api/schedulers/overview\n",
        "def get_a1111_name(scheduler, use_karras):\n",
        "    diffusers_to_a1111_map = {\n",
        "        \"DPMSolverMultistepScheduler\": {\n",
        "            False: \"DPM++ 2M\",\n",
        "            True: \"DPM++ 2M Karras\"\n",
        "        },\n",
        "        \"DPMSolverSinglestepScheduler\": {\n",
        "            False: \"DPM++ SDE\",\n",
        "            True: \"DPM++ SDE Karras\"\n",
        "        },\n",
        "        \"KDPM2DiscreteScheduler\": {\n",
        "            False: \"DPM2\",\n",
        "            True: \"DPM2 Karras\"\n",
        "        },\n",
        "        \"KDPM2AncestralDiscreteScheduler\": {\n",
        "            False: \"DPM2 a\",\n",
        "            True: \"DPM2 a Karras\"\n",
        "        },\n",
        "        \"EulerDiscreteScheduler\": {\n",
        "            False: \"Euler\"\n",
        "        },\n",
        "        \"EulerAncestralDiscreteScheduler\": {\n",
        "            False: \"Euler a\"\n",
        "        },\n",
        "        \"HeunDiscreteScheduler\": {\n",
        "            False: \"Heun\"\n",
        "        },\n",
        "        \"LMSDiscreteScheduler\": {\n",
        "            False: \"LMS\",\n",
        "            True: \"LMS Karras\"\n",
        "        },\n",
        "        \"DEISMultistepScheduler\": {\n",
        "            False: \"N/A\"\n",
        "        },\n",
        "        \"UniPCMultistepScheduler\": {\n",
        "            False: \"N/A\"\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return diffusers_to_a1111_map.get(scheduler, {}).get(use_karras, scheduler)\n",
        "\n",
        "\n",
        "def embed_png_info(image_path, info):\n",
        "    with PngImagePlugin.PngImageFile(image_path) as img:\n",
        "        # Embed the info as a PNG text chunk\n",
        "        meta = PngImagePlugin.PngInfo()\n",
        "        #encoded_info = b'UNICODE' + info.encode('utf-8')\n",
        "        meta.add_text(\"parameters\", info)\n",
        "        img.save(image_path, pnginfo=meta)\n",
        "\n",
        "def format_png_info(prompt, negative_prompt, steps, width, height, seed, scheduler, cfg_scale, clip_skip,**kwargs):\n",
        "    # Construct the PNG info\n",
        "    info = prompt + \"\\n\"\n",
        "    info += f\"Negative prompt: {negative_prompt}\\n\"\n",
        "    info += f\"Steps: {steps}, Size: {width}x{height}, Seed: {seed}, Sampler: {scheduler}, CFG scale: {cfg_scale}, Clip skip: {clip_skip},\\n\"\n",
        "\n",
        "    # Add any additional kwargs\n",
        "    for key, value in kwargs.items():\n",
        "        info += f\"{key}: {value}, \"\n",
        "\n",
        "    # Remove trailing comma and space\n",
        "    info = info.rstrip(\", \")\n",
        "    \n",
        "    return info\n",
        "\n",
        "def save_image_with_png_info(image_path):\n",
        "    global PROMPT\n",
        "    global NEGATIVE_PROMPT\n",
        "    global SAMPLING_STEPS\n",
        "    global WIDTH\n",
        "    global HEIGHT\n",
        "    global SEED\n",
        "    global SCHEDULER\n",
        "    global USE_KARRAS\n",
        "    global CFG_scale\n",
        "    global CLIP_SKIP\n",
        "    \n",
        "    clip_skip = CLIP_SKIP\n",
        "    if clip_skip is None:\n",
        "        clip_skip = 1\n",
        "    \n",
        "    # Save the image\n",
        "    image.save(image_path)\n",
        "\n",
        "    # Generate PNG info and embed it into the image\n",
        "    embed_png_info(image_path, \n",
        "                   format_png_info(PROMPT, NEGATIVE_PROMPT, SAMPLING_STEPS,\n",
        "                                   WIDTH, HEIGHT, SEED, get_a1111_name(SCHEDULER, USE_KARRAS), CFG_scale, \n",
        "                                   clip_skip ) )\n",
        "\n",
        "                   \n",
        "if SEED == -1:\n",
        "    SEED = random.randint(1,10000000000)\n",
        "generator = torch.Generator(device=\"cuda\").manual_seed(SEED)\n",
        "sc = f'''pipe.scheduler = {SCHEDULER}.from_config(pipe.scheduler.config, use_karras_sigmas={USE_KARRAS})'''\n",
        "exec(sc)\n",
        "final_images=[]\n",
        "if NUMBER_OF_IMAGES != 1:\n",
        "    for i in range(NUMBER_OF_IMAGES):\n",
        "        SEED = random.randint(1,10000000000)\n",
        "        print(f'SEED : {SEED}')\n",
        "        generator = torch.Generator(device=\"cuda\").manual_seed(SEED)\n",
        "        image = pipe(PROMPT,negative_prompt = NEGATIVE_PROMPT,clip_skip=CLIP_SKIP,generator=generator,width = WIDTH , height = HEIGHT,num_inference_steps=SAMPLING_STEPS,guidance_scale = CFG_scale).images[0]\n",
        "        final_images.append(image)\n",
        "        save_image_with_png_info(f'outputs/{PROMPT[:10]}_{SEED}.png')\n",
        "    f_images = make_image_grid(final_images, 1, len(final_images))\n",
        "    display(f_images)\n",
        "else:\n",
        "    print(f'SEED : {SEED}')\n",
        "    image = pipe(PROMPT,negative_prompt = NEGATIVE_PROMPT,clip_skip=CLIP_SKIP,generator=generator,width = WIDTH , height = HEIGHT,num_inference_steps=SAMPLING_STEPS,guidance_scale = CFG_scale).images[0]\n",
        "    save_image_with_png_info(f'outputs/{PROMPT[:11]}_{SEED}_{CLIP_SKIP}_{SCHEDULER}.png')\n",
        "    display(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "njvEFp1wSseb"
      },
      "outputs": [],
      "source": [
        "# @title DOWNLOAD THE ZIP WITH ALL THE IMAGES\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "def zip_folder(folder_path, zip_filename):\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, folder_path)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "folder_to_zip = \"outputs\"\n",
        "zip_filename = \"output.zip\"\n",
        "zip_folder(folder_to_zip, zip_filename)\n",
        "print(f'Succesfully saved all the images in {zip_filename}\\nDownloading the zip.....')\n",
        "files.download(zip_filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
